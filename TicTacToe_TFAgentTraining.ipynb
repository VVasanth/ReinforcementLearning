{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VVasanth/ReinforcementLearning/blob/main/TicTacToe_TFAgentTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmINEi1AQnay",
        "outputId": "0199df14-f20f-42a5-e0d8-5eb5b8497f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym>=0.21.0\n",
            "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
            "\u001b[K     |████████████████████████████████| 696 kB 4.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gym-notices>=0.0.4\n",
            "  Downloading gym_notices-0.0.7-py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21.0) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21.0) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.21.0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.21.0) (3.8.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793155 sha256=7115350d9dbc944c22befa5852135177c8b6a7efd94e7630a9da77ba27a8b120\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0e/54/63d9f3d16ddf0fec1622e90d28140df5e6016bcf8ea920037d\n",
            "Successfully built gym\n",
            "Installing collected packages: gym-notices, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.24.1 gym-notices-0.0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf-agents\n",
            "  Downloading tf_agents-0.13.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (4.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 122 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents) (7.1.2)\n",
            "Requirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.16.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[K     |████████████████████████████████| 624 kB 66.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.8.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.1.7)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.5.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (4.4.2)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697662 sha256=cfb9050dfd81c6c854ba76b10455f866d6bc3bf44eaf205874a8035b740cfa73\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/7e/16/4d727df048fdb96518ec5c02266e55b98bc398837353852a6a\n",
            "Successfully built gym\n",
            "Installing collected packages: pygame, gym, tf-agents\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.24.1\n",
            "    Uninstalling gym-0.24.1:\n",
            "      Successfully uninstalled gym-0.24.1\n",
            "Successfully installed gym-0.23.0 pygame-2.1.0 tf-agents-0.13.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,040 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,521 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,297 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,861 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,294 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [29.8 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,006 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [12.2 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.8 kB]\n",
            "Fetched 12.3 MB in 3s (4,548 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,271 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n",
            "Fetched 784 kB in 1s (883 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155639 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imageio==2.4.0\n",
            "  Downloading imageio-2.4.0.tar.gz (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (7.1.2)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.0-py3-none-any.whl size=3303895 sha256=ed9b928bf2040e1f18fad207282b9b2e37edaede97a85def46aba8af42185cde\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/de/2f/6c5a75120d68a2c3138120c8d0ce1c6f9483a4b96307986bf2\n",
            "Successfully built imageio\n",
            "Installing collected packages: imageio\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed imageio-2.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.7/dist-packages (0.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (4.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.21.6)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (2.1.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (7.1.2)\n",
            "Requirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.16.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (3.17.3)\n",
            "Collecting tensorflow~=2.9.0\n",
            "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 6.1 kB/s \n",
            "\u001b[?25hCollecting rlds\n",
            "  Downloading rlds-0.1.4-py3-none-manylinux2010_x86_64.whl (37 kB)\n",
            "Collecting dm-reverb~=0.8.0\n",
            "  Downloading dm_reverb-0.8.0-cp37-cp37m-manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 17.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.8.0->tf-agents[reverb]) (0.1.7)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.8.0->tf-agents[reverb]) (1.3.9)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (4.11.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (3.8.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (14.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.46.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (0.26.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.6.3)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (3.3.0)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (1.1.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-agents[reverb]) (21.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.9.0->tf-agents[reverb]) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.9.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.3.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-agents[reverb]) (3.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow~=2.9.0->tf-agents[reverb]) (3.0.9)\n",
            "Installing collected packages: gast, tensorflow-estimator, tensorboard, keras, flatbuffers, tensorflow, rlds, dm-reverb\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "Successfully installed dm-reverb-0.8.0 flatbuffers-1.12 gast-0.4.0 keras-2.9.0 rlds-0.1.4 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"gym>=0.21.0\"\n",
        "!pip install tf-agents\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTr21qPlL_i1",
        "outputId": "b9e6da5a-63bf-4270-a368-42374aa5d074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gew6JYZ-QuVP"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import itertools\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "import numpy as np\n",
        "import random\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf_dfO4NjW1a",
        "outputId": "2f70be22-083f-451e-8c39-e4fe6d9e56eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.9.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZq_8XBlT_xE"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "import numpy as np\n",
        "import random\n",
        "from itertools import product\n",
        "\n",
        "class TicTacToeAgentEnv(py_environment.PyEnvironment):\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.board_rows = 3\n",
        "        self.board_cols = 3\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=8, name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(9,), dtype=np.float64, name='observation')\n",
        "        self._step_counter = 0\n",
        "        self.board = np.zeros((self.board_rows, self.board_cols))\n",
        "        self.name = name\n",
        "        self.lr = 0.2\n",
        "        self.exp_rate = 0.3\n",
        "        self.decay_gamma = 0.9\n",
        "        self.episode_ended = False\n",
        "        self.base_positions = self.generatePositions(self.board_rows)\n",
        "        self._no_of_tries = 10\n",
        "        self._states = np.zeros(self.board_rows * self.board_cols)\n",
        "\n",
        "    def generatePositions(self, size):\n",
        "        # initialize N\n",
        "        # All possible N combination tuples\n",
        "        # Using list comprehension + product()\n",
        "        res = [ele for ele in product(range(0, size), repeat=size-1)]\n",
        "        return res\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def get_obs(self):\n",
        "        return self._states\n",
        "\n",
        "    def get_availablePositions(self):\n",
        "        positions = []\n",
        "        for i in range(self.board_rows):\n",
        "            for j in range(self.board_cols):\n",
        "                if self.board[i, j] == 0:\n",
        "                    positions.append((i, j))  # need to be tuple\n",
        "        return positions\n",
        "\n",
        "    def _reset(self):\n",
        "        self.board = np.zeros((self.board_rows, self.board_cols))\n",
        "        self._states = np.zeros(self.board_rows*self.board_cols)\n",
        "        self.episode_ended = False\n",
        "        self._step_counter = 0\n",
        "        return ts.restart(self.get_obs())\n",
        "\n",
        "    def updateState(self,action, position, value):\n",
        "        self._states[action] = 1\n",
        "        self.board[position] = value\n",
        "        status = self.checkGameStatus()\n",
        "        return status\n",
        "\n",
        "    def checkGameStatus(self):\n",
        "        # row\n",
        "        for i in range(self.board_rows):\n",
        "            if sum(self.board[i, :]) == 3:\n",
        "                self.episode_ended = True\n",
        "                return 1\n",
        "            if sum(self.board[i, :]) == -3:\n",
        "                self.episode_ended = True\n",
        "                return -1\n",
        "        # col\n",
        "        for i in range(self.board_cols):\n",
        "            if sum(self.board[:, i]) == 3:\n",
        "                self.episode_ended = True\n",
        "                return 1\n",
        "            if sum(self.board[:, i]) == -3:\n",
        "                self.episode_ended = True\n",
        "                return -1\n",
        "        # diagonal\n",
        "        diag_sum1 = sum([self.board[i, i] for i in range(self.board_cols)])\n",
        "        diag_sum2 = sum([self.board[i, self.board_cols - i - 1] for i in range(self.board_cols)])\n",
        "        diag_sum = max(diag_sum1, diag_sum2)\n",
        "        if diag_sum == 3:\n",
        "            self.episode_ended = True\n",
        "            return 1\n",
        "        if diag_sum == -3:\n",
        "            self.episode_ended = True\n",
        "            return -1\n",
        "\n",
        "        # tie\n",
        "        # no available positions\n",
        "        if len(self.get_availablePositions()) == 0:\n",
        "            self.episode_ended = True\n",
        "            return -0.8\n",
        "        # not end\n",
        "        self.episode_ended = False\n",
        "        return None\n",
        "\n",
        "    def checkPositionExists(self, position, avlPositions):\n",
        "        retValue = False\n",
        "        for i in range(len(avlPositions)):\n",
        "            posVal = avlPositions[i]\n",
        "            if (posVal == position):\n",
        "                retValue = True\n",
        "                return retValue\n",
        "        return retValue\n",
        "\n",
        "    def _step(self, action):\n",
        "        updateStateFlag = 0\n",
        "\n",
        "        avl_positions = self.get_availablePositions()\n",
        "        if (len(avl_positions) == 0):\n",
        "            self.episode_ended = True\n",
        "\n",
        "        if self.episode_ended:\n",
        "            # The last action ended the episode. Ignore the current action and start\n",
        "            # a new episode.\n",
        "            #print(\"# of steps performed : \" + str(self._step_counter))\n",
        "            self._reset()\n",
        "\n",
        "        position = self.base_positions[action]\n",
        "        if(self._step_counter==self._no_of_tries):\n",
        "            self.episode_ended = True\n",
        "            reward = -1\n",
        "            return ts.termination(self.get_obs(), reward)\n",
        "        self._step_counter = self._step_counter + 1\n",
        "\n",
        "\n",
        "        if self.checkPositionExists(position, avl_positions):\n",
        "            gameStatus = self.updateState(action, position, 1)\n",
        "            if (gameStatus == -1):\n",
        "                reward = -1\n",
        "                self.episode_ended = True\n",
        "                return ts.termination(self.get_obs(), reward)\n",
        "            elif (gameStatus == 1):\n",
        "                reward = 1\n",
        "                self.episode_ended = True\n",
        "                return ts.termination(self.get_obs(), reward)\n",
        "            elif (gameStatus == -0.8):\n",
        "                reward = -0.8\n",
        "                self.episode_ended = True\n",
        "                return ts.termination(self.get_obs(), reward)\n",
        "            elif (gameStatus == None):\n",
        "                reward = 0.1\n",
        "                avlPositions = self.get_availablePositions()\n",
        "                oppPosition = random.choice(avlPositions)\n",
        "                oppAction = self.base_positions.index(oppPosition)\n",
        "                gameStatus = self.updateState(oppAction,oppPosition, -1)\n",
        "                if (gameStatus == -1):\n",
        "                    reward = -1\n",
        "                    self.episode_ended = True\n",
        "                    return ts.termination(self.get_obs(), reward=reward)\n",
        "                else:\n",
        "                    return ts.transition(self.get_obs(), reward=reward, discount=1.0)\n",
        "        else:\n",
        "            \n",
        "            #print(\"chosen non-existent action at the step \" + str(self._step_counter))\n",
        "            reward = -1\n",
        "            return ts.termination(self.get_obs(), reward=reward)\n",
        "            #return ts.transition(self.get_obs(), reward=reward, discount=1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_hsNvefm9gx"
      },
      "outputs": [],
      "source": [
        "tictacplayer1 = TicTacToeAgentEnv(\"p1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDwvvSbVm96n"
      },
      "outputs": [],
      "source": [
        "from tf_agents.environments import utils\n",
        "utils.validate_py_environment(tictacplayer1, episodes=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfo73xl8U4G4"
      },
      "outputs": [],
      "source": [
        "tictacplayer2 = TicTacToeAgentEnv(\"p2\")\n",
        "utils.validate_py_environment(tictacplayer2, episodes=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUjVeELY_YfN"
      },
      "outputs": [],
      "source": [
        "from tf_agents.environments import tf_py_environment\n",
        "\n",
        "#train_py_env = suite_gym.wrap_env(train_qt_env)\n",
        "tictacplayer1_tf = tf_py_environment.TFPyEnvironment(tictacplayer1)\n",
        "\n",
        "#eval_py_env = suite_gym.wrap_env(eval_qt_env)\n",
        "tictacplayer2_tf = tf_py_environment.TFPyEnvironment(tictacplayer2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSUV-MJYVTIs"
      },
      "outputs": [],
      "source": [
        "from tf_agents.policies import random_tf_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(tictacplayer1_tf.time_step_spec(), tictacplayer1_tf.action_spec())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STqZlc6aYaM_"
      },
      "outputs": [],
      "source": [
        "time_step = tictacplayer1_tf._reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rnC75SNYe76",
        "outputId": "6b4cf171-50d4-40d4-d756-2beae9eb1bb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
              " 'observation': <tf.Tensor: shape=(1, 9), dtype=float64, numpy=array([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])>,\n",
              " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
              " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjztuBbgYhkM"
      },
      "outputs": [],
      "source": [
        "action_step = random_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p60IUAubYk2B",
        "outputId": "f48b2922-74c3-4ca3-d8c2-a18e00d88529"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "action_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aK_BdBMYmST"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3 \n",
        "num_eval_episodes = 10\n",
        "replay_buffer_max_length = 100000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jogm_RkJYo0D"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.networks import sequential\n",
        "\n",
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(tictacplayer1_tf.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDrUfWmCZZQp",
        "outputId": "63ba75cf-c3f6-4add-934d-6693bf17bd5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsSobThMYrPW"
      },
      "outputs": [],
      "source": [
        "from tf_agents.agents.dqn import dqn_agent\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    tictacplayer1_tf.time_step_spec(),\n",
        "    tictacplayer1_tf.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP6T81TDY1by"
      },
      "outputs": [],
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6aPon6vc1-R"
      },
      "outputs": [],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(tictacplayer1_tf.time_step_spec(), tictacplayer1_tf.action_spec())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hctI5hmnc3zw"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=5):\n",
        "\n",
        "  total_wins = 0\n",
        "  total_loss = 0\n",
        "  total_ties = 0\n",
        "  no_of_steps_in_win = 0\n",
        "  no_of_steps_in_loss = 0\n",
        "  avg_steps_in_win = 0\n",
        "  avg_steps_in_loss = 0\n",
        "\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = environment._reset()\n",
        "    no_of_steps = 0\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      no_of_steps = no_of_steps + 1\n",
        "      time_step = environment._step(action_step.action)\n",
        "      if(time_step.reward==1):\n",
        "        total_wins = total_wins + 1\n",
        "        no_of_steps_in_win = no_of_steps_in_win + no_of_steps\n",
        "      elif(time_step.reward==-1):\n",
        "        total_loss = total_loss + 1\n",
        "        no_of_steps_in_loss = no_of_steps_in_loss + no_of_steps\n",
        "      elif(time_step.reward==-0.8):\n",
        "        total_ties = total_ties + 1\n",
        "\n",
        "  if(total_wins!=0):\n",
        "    avg_steps_in_win = no_of_steps_in_win/total_wins\n",
        "  \n",
        "  if(total_loss!=0):\n",
        "    avg_steps_in_loss = no_of_steps_in_loss/total_loss\n",
        "\n",
        "  return total_wins, total_loss, total_ties, avg_steps_in_win, avg_steps_in_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aB-HbxQc__U",
        "outputId": "00f10a09-3e65-49c9-8f94-32ae77f223aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 15, 0, 3.6, 3.066666666666667)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_avg_return(tictacplayer2_tf, random_policy, num_episodes=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKHKt-AmdYKy"
      },
      "outputs": [],
      "source": [
        "from tf_agents.replay_buffers import TFUniformReplayBuffer\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnx_L2_gdg_9"
      },
      "outputs": [],
      "source": [
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=tictacplayer1_tf.batch_size,\n",
        "    max_length=100_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_pzgIRkdjCJ"
      },
      "outputs": [],
      "source": [
        "tictacplayer1_tf._reset()\n",
        "\n",
        "init_driver = DynamicStepDriver(\n",
        "    tictacplayer1_tf,\n",
        "    random_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps=2_500)\n",
        "final_time_step, final_policy_state = init_driver.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZiAdri0drsn",
        "outputId": "a9175cc6-e9ba-4c11-f9d0-b1dc4144f3b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-26-38750cf20376>:1: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Trajectory(\n",
              "{'action': <tf.Tensor: shape=(1, 6488), dtype=int32, numpy=array([[0, 2, 3, ..., 3, 4, 6]], dtype=int32)>,\n",
              " 'discount': <tf.Tensor: shape=(1, 6488), dtype=float32, numpy=array([[1., 1., 0., ..., 0., 1., 0.]], dtype=float32)>,\n",
              " 'next_step_type': <tf.Tensor: shape=(1, 6488), dtype=int32, numpy=array([[1, 1, 2, ..., 2, 1, 2]], dtype=int32)>,\n",
              " 'observation': <tf.Tensor: shape=(1, 6488, 9), dtype=float64, numpy=\n",
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 1., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 1., 0., 0.],\n",
              "        [0., 0., 0., ..., 1., 0., 0.],\n",
              "        [0., 1., 0., ..., 1., 0., 0.]]])>,\n",
              " 'policy_info': (),\n",
              " 'reward': <tf.Tensor: shape=(1, 6488), dtype=float32, numpy=array([[ 0.1,  0.1, -1. , ..., -1. ,  0.1, -1. ]], dtype=float32)>,\n",
              " 'step_type': <tf.Tensor: shape=(1, 6488), dtype=int32, numpy=array([[0, 1, 1, ..., 2, 2, 1]], dtype=int32)>})"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "replay_buffer.gather_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYsiikRwd1ZM",
        "outputId": "169413af-21fa-4917-8f3a-8ccb8629ce2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ]
        }
      ],
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=64,\n",
        "    num_steps=2).prefetch(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPPntUdkd4Zr"
      },
      "outputs": [],
      "source": [
        "iterator = iter(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yywRFp2_d7Lh"
      },
      "outputs": [],
      "source": [
        "num_iterations = 1_000_000   # less intelligence, more persistance; 24x7 player\n",
        "save_interval = 50_000\n",
        "eval_interval = 10_000\n",
        "log_interval = 10_000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkSJgCe9d82X"
      },
      "outputs": [],
      "source": [
        "# Create a driver to collect experience.\n",
        "collect_driver = DynamicStepDriver(\n",
        "    tictacplayer1_tf,\n",
        "    agent.collect_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps=4) # collect 4 steps for each training iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSHO2KKYd-R1"
      },
      "outputs": [],
      "source": [
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "collect_driver.run = common.function(collect_driver.run)\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "no_of_wins, no_of_losses, no_of_ties, avg_steps_to_win, avg_steps_to_loss = compute_avg_return(tictacplayer2_tf, agent.policy, num_episodes=1)\n",
        "no_of_wins_arr = np.array([no_of_wins])\n",
        "no_of_losses_arr = np.array([no_of_losses])\n",
        "no_of_ties_arr = np.array([no_of_ties])\n",
        "avg_steps_to_win_arr = np.array([avg_steps_to_win])\n",
        "avg_steps_to_loss_arr = np.array([avg_steps_to_loss])\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = None\n",
        "policy_state = agent.collect_policy.get_initial_state(tictacplayer1_tf.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY9vyXfDeQ13",
        "outputId": "23cc9f5f-6257-4da9-f83e-c7c681b54cf4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "no_of_wins_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzlfrg3IOh-2",
        "outputId": "f564b7c3-0f21-4e3b-a39f-d478fb7cf8f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "no_of_losses_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYe6JWFPey9I",
        "outputId": "df8e21d0-09b5-41a9-b2bd-51947c75c7db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "no_of_ties_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjDfl1fa84ar"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "basedir = '/content/drive/My Drive/Colab Notebooks/ReinforcementLearning/TicTacToe'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "5pFmRj049A_S",
        "outputId": "eb863eb1-341b-4927-a2c7-16d248955dba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/ReinforcementLearning/TicTacToe'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basedir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka3cgXN9fXQl"
      },
      "outputs": [],
      "source": [
        "import tensorflow_probability\n",
        "from tf_agents.policies import policy_saver\n",
        "tf_policy_saver = policy_saver.PolicySaver(agent.policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz991RdzgMyU"
      },
      "outputs": [],
      "source": [
        "policy_dir = os.path.join(basedir, 'policy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "i7r_OuUDgjEU",
        "outputId": "3e935daa-1280-4217-a443-6252a251be29"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/ReinforcementLearning/TicTacToe/policy'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "80TXHcOgeJ70",
        "outputId": "0bdebdd6-bb43-489e-f014-84fde37f19aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
            " step 10000step = 10000: loss = 0.12603648006916046\n",
            "****************************************************\n",
            "step = 10000: # of wins = 13\n",
            "# of losses = 6\n",
            "# of ties = 6\n",
            "Avg steps to win = 4.461538461538462\n",
            "Avg steps to loss = 3.8333333333333335\n",
            " step 20000step = 20000: loss = 0.1754954606294632\n",
            "****************************************************\n",
            "step = 20000: # of wins = 15\n",
            "# of losses = 2\n",
            "# of ties = 8\n",
            "Avg steps to win = 4.2\n",
            "Avg steps to loss = 3.5\n",
            " step 30000step = 30000: loss = 0.2097139060497284\n",
            "****************************************************\n",
            "step = 30000: # of wins = 18\n",
            "# of losses = 3\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.388888888888889\n",
            "Avg steps to loss = 4.0\n",
            " step 40000step = 40000: loss = 0.11431829631328583\n",
            "****************************************************\n",
            "step = 40000: # of wins = 22\n",
            "# of losses = 2\n",
            "# of ties = 1\n",
            "Avg steps to win = 3.1818181818181817\n",
            "Avg steps to loss = 4.0\n",
            " step 50000step = 50000: loss = 0.12809184193611145\n",
            "****************************************************\n",
            "step = 50000: # of wins = 22\n",
            "# of losses = 1\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.1363636363636362\n",
            "Avg steps to loss = 3.0\n",
            " step 60000step = 60000: loss = 0.13585394620895386\n",
            "****************************************************\n",
            "step = 60000: # of wins = 17\n",
            "# of losses = 1\n",
            "# of ties = 7\n",
            "Avg steps to win = 3.7058823529411766\n",
            "Avg steps to loss = 4.0\n",
            " step 70000step = 70000: loss = 0.08829601854085922\n",
            "****************************************************\n",
            "step = 70000: # of wins = 21\n",
            "# of losses = 2\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.380952380952381\n",
            "Avg steps to loss = 4.0\n",
            " step 80000step = 80000: loss = 0.22325898706912994\n",
            "****************************************************\n",
            "step = 80000: # of wins = 19\n",
            "# of losses = 3\n",
            "# of ties = 3\n",
            "Avg steps to win = 3.3157894736842106\n",
            "Avg steps to loss = 4.0\n",
            " step 90000step = 90000: loss = 0.07422907650470734\n",
            "****************************************************\n",
            "step = 90000: # of wins = 19\n",
            "# of losses = 1\n",
            "# of ties = 5\n",
            "Avg steps to win = 3.4210526315789473\n",
            "Avg steps to loss = 4.0\n",
            " step 100000step = 100000: loss = 0.14415904879570007\n",
            "****************************************************\n",
            "step = 100000: # of wins = 18\n",
            "# of losses = 2\n",
            "# of ties = 5\n",
            "Avg steps to win = 3.4444444444444446\n",
            "Avg steps to loss = 3.0\n",
            " step 110000step = 110000: loss = 0.1434726119041443\n",
            "****************************************************\n",
            "step = 110000: # of wins = 24\n",
            "# of losses = 0\n",
            "# of ties = 1\n",
            "Avg steps to win = 3.4583333333333335\n",
            "Avg steps to loss = 0\n",
            " step 120000step = 120000: loss = 0.18899428844451904\n",
            "****************************************************\n",
            "step = 120000: # of wins = 19\n",
            "# of losses = 3\n",
            "# of ties = 3\n",
            "Avg steps to win = 3.0526315789473686\n",
            "Avg steps to loss = 3.6666666666666665\n",
            " step 130000step = 130000: loss = 0.04859413951635361\n",
            "****************************************************\n",
            "step = 130000: # of wins = 19\n",
            "# of losses = 2\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.1578947368421053\n",
            "Avg steps to loss = 4.0\n",
            " step 140000step = 140000: loss = 0.22677582502365112\n",
            "****************************************************\n",
            "step = 140000: # of wins = 19\n",
            "# of losses = 2\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.4210526315789473\n",
            "Avg steps to loss = 4.0\n",
            " step 150000step = 150000: loss = 0.09994522482156754\n",
            "****************************************************\n",
            "step = 150000: # of wins = 22\n",
            "# of losses = 0\n",
            "# of ties = 3\n",
            "Avg steps to win = 3.0454545454545454\n",
            "Avg steps to loss = 0\n",
            " step 160000step = 160000: loss = 0.21248790621757507\n",
            "****************************************************\n",
            "step = 160000: # of wins = 17\n",
            "# of losses = 5\n",
            "# of ties = 3\n",
            "Avg steps to win = 3.2941176470588234\n",
            "Avg steps to loss = 3.8\n",
            " step 170000step = 170000: loss = 0.18129962682724\n",
            "****************************************************\n",
            "step = 170000: # of wins = 18\n",
            "# of losses = 1\n",
            "# of ties = 6\n",
            "Avg steps to win = 3.388888888888889\n",
            "Avg steps to loss = 4.0\n",
            " step 180000step = 180000: loss = 0.11572347581386566\n",
            "****************************************************\n",
            "step = 180000: # of wins = 20\n",
            "# of losses = 3\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.4\n",
            "Avg steps to loss = 3.6666666666666665\n",
            " step 190000step = 190000: loss = 0.14394402503967285\n",
            "****************************************************\n",
            "step = 190000: # of wins = 19\n",
            "# of losses = 0\n",
            "# of ties = 6\n",
            "Avg steps to win = 4.052631578947368\n",
            "Avg steps to loss = 0\n",
            " step 200000step = 200000: loss = 0.17034511268138885\n",
            "****************************************************\n",
            "step = 200000: # of wins = 22\n",
            "# of losses = 1\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.1363636363636362\n",
            "Avg steps to loss = 4.0\n",
            " step 210000step = 210000: loss = 0.18872016668319702\n",
            "****************************************************\n",
            "step = 210000: # of wins = 20\n",
            "# of losses = 0\n",
            "# of ties = 5\n",
            "Avg steps to win = 3.2\n",
            "Avg steps to loss = 0\n",
            " step 220000step = 220000: loss = 0.2198582887649536\n",
            "****************************************************\n",
            "step = 220000: # of wins = 21\n",
            "# of losses = 1\n",
            "# of ties = 3\n",
            "Avg steps to win = 3.238095238095238\n",
            "Avg steps to loss = 4.0\n",
            " step 230000step = 230000: loss = 0.1651822030544281\n",
            "****************************************************\n",
            "step = 230000: # of wins = 21\n",
            "# of losses = 2\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.7142857142857144\n",
            "Avg steps to loss = 3.5\n",
            " step 240000step = 240000: loss = 0.08839622884988785\n",
            "****************************************************\n",
            "step = 240000: # of wins = 17\n",
            "# of losses = 2\n",
            "# of ties = 6\n",
            "Avg steps to win = 3.1176470588235294\n",
            "Avg steps to loss = 4.0\n",
            " step 250000step = 250000: loss = 0.16161876916885376\n",
            "****************************************************\n",
            "step = 250000: # of wins = 21\n",
            "# of losses = 1\n",
            "# of ties = 3\n",
            "Avg steps to win = 3.238095238095238\n",
            "Avg steps to loss = 4.0\n",
            " step 260000step = 260000: loss = 0.24670562148094177\n",
            "****************************************************\n",
            "step = 260000: # of wins = 23\n",
            "# of losses = 0\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.1739130434782608\n",
            "Avg steps to loss = 0\n",
            " step 270000step = 270000: loss = 0.09040489792823792\n",
            "****************************************************\n",
            "step = 270000: # of wins = 21\n",
            "# of losses = 0\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.3333333333333335\n",
            "Avg steps to loss = 0\n",
            " step 280000step = 280000: loss = 0.08303546160459518\n",
            "****************************************************\n",
            "step = 280000: # of wins = 18\n",
            "# of losses = 1\n",
            "# of ties = 6\n",
            "Avg steps to win = 3.2222222222222223\n",
            "Avg steps to loss = 4.0\n",
            " step 290000step = 290000: loss = 0.12463545054197311\n",
            "****************************************************\n",
            "step = 290000: # of wins = 23\n",
            "# of losses = 1\n",
            "# of ties = 1\n",
            "Avg steps to win = 3.0869565217391304\n",
            "Avg steps to loss = 4.0\n",
            " step 300000step = 300000: loss = 0.14835095405578613\n",
            "****************************************************\n",
            "step = 300000: # of wins = 21\n",
            "# of losses = 0\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.3333333333333335\n",
            "Avg steps to loss = 0\n",
            " step 310000step = 310000: loss = 0.07876650989055634\n",
            "****************************************************\n",
            "step = 310000: # of wins = 18\n",
            "# of losses = 1\n",
            "# of ties = 6\n",
            "Avg steps to win = 3.1666666666666665\n",
            "Avg steps to loss = 4.0\n",
            " step 320000step = 320000: loss = 0.18233948945999146\n",
            "****************************************************\n",
            "step = 320000: # of wins = 18\n",
            "# of losses = 0\n",
            "# of ties = 7\n",
            "Avg steps to win = 3.5555555555555554\n",
            "Avg steps to loss = 0\n",
            " step 330000step = 330000: loss = 0.10277145355939865\n",
            "****************************************************\n",
            "step = 330000: # of wins = 22\n",
            "# of losses = 0\n",
            "# of ties = 3\n",
            "Avg steps to win = 3.0454545454545454\n",
            "Avg steps to loss = 0\n",
            " step 340000step = 340000: loss = 0.17075467109680176\n",
            "****************************************************\n",
            "step = 340000: # of wins = 21\n",
            "# of losses = 2\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.0476190476190474\n",
            "Avg steps to loss = 4.0\n",
            " step 350000step = 350000: loss = 0.19619137048721313\n",
            "****************************************************\n",
            "step = 350000: # of wins = 20\n",
            "# of losses = 1\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.2\n",
            "Avg steps to loss = 4.0\n",
            " step 360000step = 360000: loss = 0.19915735721588135\n",
            "****************************************************\n",
            "step = 360000: # of wins = 16\n",
            "# of losses = 4\n",
            "# of ties = 5\n",
            "Avg steps to win = 3.375\n",
            "Avg steps to loss = 3.75\n",
            " step 370000step = 370000: loss = 0.16595876216888428\n",
            "****************************************************\n",
            "step = 370000: # of wins = 22\n",
            "# of losses = 1\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.0\n",
            "Avg steps to loss = 4.0\n",
            " step 380000step = 380000: loss = 0.12423475831747055\n",
            "****************************************************\n",
            "step = 380000: # of wins = 19\n",
            "# of losses = 2\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.736842105263158\n",
            "Avg steps to loss = 4.0\n",
            " step 390000step = 390000: loss = 0.23504191637039185\n",
            "****************************************************\n",
            "step = 390000: # of wins = 18\n",
            "# of losses = 1\n",
            "# of ties = 6\n",
            "Avg steps to win = 3.7222222222222223\n",
            "Avg steps to loss = 4.0\n",
            " step 400000step = 400000: loss = 0.20375187695026398\n",
            "****************************************************\n",
            "step = 400000: # of wins = 18\n",
            "# of losses = 3\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.2222222222222223\n",
            "Avg steps to loss = 3.3333333333333335\n",
            " step 410000step = 410000: loss = 0.13175615668296814\n",
            "****************************************************\n",
            "step = 410000: # of wins = 22\n",
            "# of losses = 1\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.4545454545454546\n",
            "Avg steps to loss = 4.0\n",
            " step 420000step = 420000: loss = 0.1621772199869156\n",
            "****************************************************\n",
            "step = 420000: # of wins = 22\n",
            "# of losses = 2\n",
            "# of ties = 1\n",
            "Avg steps to win = 3.227272727272727\n",
            "Avg steps to loss = 4.0\n",
            " step 430000step = 430000: loss = 0.18987725675106049\n",
            "****************************************************\n",
            "step = 430000: # of wins = 19\n",
            "# of losses = 2\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.4210526315789473\n",
            "Avg steps to loss = 3.0\n",
            " step 440000step = 440000: loss = 0.1566721796989441\n",
            "****************************************************\n",
            "step = 440000: # of wins = 17\n",
            "# of losses = 2\n",
            "# of ties = 6\n",
            "Avg steps to win = 3.176470588235294\n",
            "Avg steps to loss = 4.0\n",
            " step 450000step = 450000: loss = 0.14931720495224\n",
            "****************************************************\n",
            "step = 450000: # of wins = 19\n",
            "# of losses = 1\n",
            "# of ties = 5\n",
            "Avg steps to win = 3.1052631578947367\n",
            "Avg steps to loss = 4.0\n",
            " step 460000step = 460000: loss = 0.13928546011447906\n",
            "****************************************************\n",
            "step = 460000: # of wins = 18\n",
            "# of losses = 2\n",
            "# of ties = 5\n",
            "Avg steps to win = 3.0555555555555554\n",
            "Avg steps to loss = 3.5\n",
            " step 470000step = 470000: loss = 0.16585436463356018\n",
            "****************************************************\n",
            "step = 470000: # of wins = 21\n",
            "# of losses = 2\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.3333333333333335\n",
            "Avg steps to loss = 3.5\n",
            " step 480000step = 480000: loss = 0.09879077970981598\n",
            "****************************************************\n",
            "step = 480000: # of wins = 21\n",
            "# of losses = 2\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.3333333333333335\n",
            "Avg steps to loss = 4.0\n",
            " step 490000step = 490000: loss = 0.1266762614250183\n",
            "****************************************************\n",
            "step = 490000: # of wins = 12\n",
            "# of losses = 6\n",
            "# of ties = 7\n",
            "Avg steps to win = 3.6666666666666665\n",
            "Avg steps to loss = 3.5\n",
            " step 500000step = 500000: loss = 0.16080863773822784\n",
            "****************************************************\n",
            "step = 500000: # of wins = 23\n",
            "# of losses = 1\n",
            "# of ties = 1\n",
            "Avg steps to win = 3.4347826086956523\n",
            "Avg steps to loss = 4.0\n",
            " step 510000step = 510000: loss = 0.12477460503578186\n",
            "****************************************************\n",
            "step = 510000: # of wins = 23\n",
            "# of losses = 2\n",
            "# of ties = 0\n",
            "Avg steps to win = 3.1739130434782608\n",
            "Avg steps to loss = 4.0\n",
            " step 520000step = 520000: loss = 0.16948986053466797\n",
            "****************************************************\n",
            "step = 520000: # of wins = 11\n",
            "# of losses = 6\n",
            "# of ties = 8\n",
            "Avg steps to win = 4.7272727272727275\n",
            "Avg steps to loss = 3.5\n",
            " step 530000step = 530000: loss = 0.14053857326507568\n",
            "****************************************************\n",
            "step = 530000: # of wins = 13\n",
            "# of losses = 4\n",
            "# of ties = 8\n",
            "Avg steps to win = 3.923076923076923\n",
            "Avg steps to loss = 4.0\n",
            " step 540000step = 540000: loss = 0.08183997869491577\n",
            "****************************************************\n",
            "step = 540000: # of wins = 19\n",
            "# of losses = 3\n",
            "# of ties = 3\n",
            "Avg steps to win = 3.0\n",
            "Avg steps to loss = 3.6666666666666665\n",
            " step 550000step = 550000: loss = 0.10554246604442596\n",
            "****************************************************\n",
            "step = 550000: # of wins = 14\n",
            "# of losses = 3\n",
            "# of ties = 8\n",
            "Avg steps to win = 4.0\n",
            "Avg steps to loss = 3.6666666666666665\n",
            " step 560000step = 560000: loss = 0.08704397082328796\n",
            "****************************************************\n",
            "step = 560000: # of wins = 21\n",
            "# of losses = 0\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.238095238095238\n",
            "Avg steps to loss = 0\n",
            " step 570000step = 570000: loss = 0.22100992500782013\n",
            "****************************************************\n",
            "step = 570000: # of wins = 24\n",
            "# of losses = 0\n",
            "# of ties = 1\n",
            "Avg steps to win = 3.2916666666666665\n",
            "Avg steps to loss = 0\n",
            " step 580000step = 580000: loss = 0.15512678027153015\n",
            "****************************************************\n",
            "step = 580000: # of wins = 22\n",
            "# of losses = 2\n",
            "# of ties = 1\n",
            "Avg steps to win = 3.1363636363636362\n",
            "Avg steps to loss = 4.0\n",
            " step 590000step = 590000: loss = 0.16153740882873535\n",
            "****************************************************\n",
            "step = 590000: # of wins = 15\n",
            "# of losses = 4\n",
            "# of ties = 6\n",
            "Avg steps to win = 4.0\n",
            "Avg steps to loss = 4.0\n",
            " step 600000step = 600000: loss = 0.13364824652671814\n",
            "****************************************************\n",
            "step = 600000: # of wins = 16\n",
            "# of losses = 1\n",
            "# of ties = 8\n",
            "Avg steps to win = 4.0625\n",
            "Avg steps to loss = 3.0\n",
            " step 610000step = 610000: loss = 0.22299039363861084\n",
            "****************************************************\n",
            "step = 610000: # of wins = 17\n",
            "# of losses = 2\n",
            "# of ties = 6\n",
            "Avg steps to win = 3.3529411764705883\n",
            "Avg steps to loss = 4.0\n",
            " step 620000step = 620000: loss = 0.20800268650054932\n",
            "****************************************************\n",
            "step = 620000: # of wins = 25\n",
            "# of losses = 0\n",
            "# of ties = 0\n",
            "Avg steps to win = 3.28\n",
            "Avg steps to loss = 0\n",
            " step 630000step = 630000: loss = 0.19787302613258362\n",
            "****************************************************\n",
            "step = 630000: # of wins = 17\n",
            "# of losses = 0\n",
            "# of ties = 8\n",
            "Avg steps to win = 3.1176470588235294\n",
            "Avg steps to loss = 0\n",
            " step 640000step = 640000: loss = 0.1589384377002716\n",
            "****************************************************\n",
            "step = 640000: # of wins = 22\n",
            "# of losses = 2\n",
            "# of ties = 1\n",
            "Avg steps to win = 3.272727272727273\n",
            "Avg steps to loss = 3.5\n",
            " step 650000step = 650000: loss = 0.16962626576423645\n",
            "****************************************************\n",
            "step = 650000: # of wins = 23\n",
            "# of losses = 2\n",
            "# of ties = 0\n",
            "Avg steps to win = 3.1739130434782608\n",
            "Avg steps to loss = 3.5\n",
            " step 660000step = 660000: loss = 0.13002437353134155\n",
            "****************************************************\n",
            "step = 660000: # of wins = 21\n",
            "# of losses = 2\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.238095238095238\n",
            "Avg steps to loss = 4.0\n",
            " step 670000step = 670000: loss = 0.15751993656158447\n",
            "****************************************************\n",
            "step = 670000: # of wins = 21\n",
            "# of losses = 1\n",
            "# of ties = 3\n",
            "Avg steps to win = 3.4285714285714284\n",
            "Avg steps to loss = 4.0\n",
            " step 680000step = 680000: loss = 0.12020529806613922\n",
            "****************************************************\n",
            "step = 680000: # of wins = 16\n",
            "# of losses = 1\n",
            "# of ties = 8\n",
            "Avg steps to win = 4.0\n",
            "Avg steps to loss = 3.0\n",
            " step 690000step = 690000: loss = 0.1286955177783966\n",
            "****************************************************\n",
            "step = 690000: # of wins = 23\n",
            "# of losses = 0\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.347826086956522\n",
            "Avg steps to loss = 0\n",
            " step 700000step = 700000: loss = 0.10974843055009842\n",
            "****************************************************\n",
            "step = 700000: # of wins = 21\n",
            "# of losses = 1\n",
            "# of ties = 3\n",
            "Avg steps to win = 3.5238095238095237\n",
            "Avg steps to loss = 4.0\n",
            " step 710000step = 710000: loss = 0.042764969170093536\n",
            "****************************************************\n",
            "step = 710000: # of wins = 21\n",
            "# of losses = 2\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.6666666666666665\n",
            "Avg steps to loss = 4.0\n",
            " step 720000step = 720000: loss = 0.19446802139282227\n",
            "****************************************************\n",
            "step = 720000: # of wins = 20\n",
            "# of losses = 0\n",
            "# of ties = 5\n",
            "Avg steps to win = 3.45\n",
            "Avg steps to loss = 0\n",
            " step 730000step = 730000: loss = 0.09684097021818161\n",
            "****************************************************\n",
            "step = 730000: # of wins = 17\n",
            "# of losses = 4\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.176470588235294\n",
            "Avg steps to loss = 4.0\n",
            " step 740000step = 740000: loss = 0.1557006984949112\n",
            "****************************************************\n",
            "step = 740000: # of wins = 22\n",
            "# of losses = 2\n",
            "# of ties = 1\n",
            "Avg steps to win = 3.272727272727273\n",
            "Avg steps to loss = 3.5\n",
            " step 750000step = 750000: loss = 0.1092851385474205\n",
            "****************************************************\n",
            "step = 750000: # of wins = 21\n",
            "# of losses = 0\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.0952380952380953\n",
            "Avg steps to loss = 0\n",
            " step 760000step = 760000: loss = 0.19336402416229248\n",
            "****************************************************\n",
            "step = 760000: # of wins = 20\n",
            "# of losses = 4\n",
            "# of ties = 1\n",
            "Avg steps to win = 3.1\n",
            "Avg steps to loss = 3.75\n",
            " step 770000step = 770000: loss = 0.15715262293815613\n",
            "****************************************************\n",
            "step = 770000: # of wins = 22\n",
            "# of losses = 1\n",
            "# of ties = 2\n",
            "Avg steps to win = 3.1363636363636362\n",
            "Avg steps to loss = 4.0\n",
            " step 780000step = 780000: loss = 0.169349804520607\n",
            "****************************************************\n",
            "step = 780000: # of wins = 18\n",
            "# of losses = 3\n",
            "# of ties = 4\n",
            "Avg steps to win = 3.111111111111111\n",
            "Avg steps to loss = 4.0\n",
            " step 783899"
          ]
        }
      ],
      "source": [
        "num_iterations = 1_000_000\n",
        "while True:\n",
        "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "    time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "\n",
        "    step = agent.train_step_counter.numpy()\n",
        "    print(f'\\r step {step}', end='')\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "        no_of_wins, no_of_losses, no_of_ties, avg_steps_to_win, avg_steps_to_loss = compute_avg_return(tictacplayer2_tf, agent.policy, num_episodes=25)\n",
        "        print('****************************************************')\n",
        "        print('step = {0}: # of wins = {1}'.format(step, no_of_wins))\n",
        "        print('# of losses = {1}'.format(step, no_of_losses))\n",
        "        print('# of ties = {1}'.format(step, no_of_ties))\n",
        "        print('Avg steps to win = {1}'.format(step, avg_steps_to_win))\n",
        "        print('Avg steps to loss = {1}'.format(step, avg_steps_to_loss))\n",
        "\n",
        "        no_of_wins_arr = np.append(no_of_wins_arr, no_of_wins)\n",
        "        no_of_losses_arr = np.append(no_of_losses_arr, no_of_losses)\n",
        "        no_of_ties_arr = np.append(no_of_ties_arr, no_of_ties)\n",
        "        avg_steps_to_win_arr = np.append(avg_steps_to_win_arr, avg_steps_to_win)\n",
        "        avg_steps_to_loss_arr = np.append(avg_steps_to_loss_arr, avg_steps_to_loss)\n",
        "        #train_checkpointer.save(global_step)\n",
        "\n",
        "    # if step % save_interval == 0:\n",
        "    #     save_checkpoint_to_local()\n",
        "\n",
        "    if step > num_iterations:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2Ta16Wde3cK"
      },
      "outputs": [],
      "source": [
        "tf_policy_saver.save(policy_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKXJhYEDfLGy"
      },
      "outputs": [],
      "source": [
        "saved_policy = tf.saved_model.load(policy_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6xiNEREtACa"
      },
      "outputs": [],
      "source": [
        "action_step = saved_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n4rgheutJFW"
      },
      "outputs": [],
      "source": [
        "action_step.action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT8LE5yTgvmD"
      },
      "outputs": [],
      "source": [
        "compute_avg_return(tictacplayer2_tf, saved_policy, num_episodes=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKCvfoUvB-Bp"
      },
      "outputs": [],
      "source": [
        "no_of_wins_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPz7xlB-MUI0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# data to be plotted\n",
        "x = np.arange(1, 12)\n",
        " \n",
        "# plotting\n",
        "plt.title(\"Line graph\")\n",
        "plt.xlabel(\"X axis\")\n",
        "plt.ylabel(\"Y axis\")\n",
        "plt.plot(x, no_of_wins_arr, color =\"green\")\n",
        "plt.plot(x, no_of_losses_arr, color =\"blue\")\n",
        "plt.plot(x, no_of_ties_arr, color =\"yellow\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tn2F94SgJWV"
      },
      "outputs": [],
      "source": [
        "def generatePositions(size):\n",
        "        # initialize N\n",
        "        # All possible N combination tuples\n",
        "        # Using list comprehension + product()\n",
        "        res = [ele for ele in product(range(0, size), repeat=size-1)]\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-3wqS0ThZQd"
      },
      "outputs": [],
      "source": [
        "def showBoard(board):\n",
        "    # p1: x  p2: o\n",
        "    for i in range(0, 3):\n",
        "        print('-------------')\n",
        "        out = '| '\n",
        "        for j in range(0, 3):\n",
        "            if board[i, j] == 1:\n",
        "                token = 'x'\n",
        "            if board[i, j] == -1:\n",
        "                token = 'o'\n",
        "            if board[i, j] == 0:\n",
        "                token = ' '\n",
        "            out += token + ' | '\n",
        "        print(out)\n",
        "    print('-------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUiXaQAuh-z4"
      },
      "outputs": [],
      "source": [
        "def updateAction(board, action):\n",
        "    board[position] = -1\n",
        "    return board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVfT2MRdjDmV"
      },
      "outputs": [],
      "source": [
        "def get_availablePositions(board):\n",
        "        positions = []\n",
        "        for i in range(board.shape[0]):\n",
        "            for j in range(board.shape[1]):\n",
        "                if board[i, j] == 0:\n",
        "                    positions.append((i, j))  # need to be tuple\n",
        "        return positions\n",
        "\n",
        "\n",
        "def checkGameStatus(board):\n",
        "        youWinMsg = \"You Win!!!!\"\n",
        "        youLoseMsg = \"You Lose :-(\"\n",
        "        itsTieMsg = \"Its a Tie !!!\"\n",
        "        # row\n",
        "        for i in range(board.shape[0]):\n",
        "            if sum(board[i, :]) == 3:                \n",
        "                return youLoseMsg\n",
        "            if sum(board[i, :]) == -3:                \n",
        "                return youWinMsg\n",
        "        # col\n",
        "        for i in range(board.shape[1]):\n",
        "            if sum(board[:, i]) == 3:                \n",
        "                return youLoseMsg\n",
        "            if sum(board[:, i]) == -3:              \n",
        "                return youWinMsg\n",
        "        # diagonal\n",
        "        diag_sum1 = sum([board[i, i] for i in range(board.shape[1])])\n",
        "        diag_sum2 = sum([board[i, board_cols - i - 1] for i in range(board.shape[1])])\n",
        "        diag_sum = max(diag_sum1, diag_sum2)\n",
        "        if diag_sum == 3:            \n",
        "            return youLoseMsg\n",
        "        if diag_sum == -3:\n",
        "            return youWinMsg\n",
        "\n",
        "        # tie\n",
        "        # no available positions\n",
        "        if len(get_availablePositions(board)) == 0:\n",
        "            return itsTieMsg\n",
        "        # not end\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45QpbpEgk_n6"
      },
      "outputs": [],
      "source": [
        "def genTimeStep(state):\n",
        "  return ts.transition(state, reward=1, discount=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "Pu_0j5JThi4p",
        "outputId": "4005e9f5-6590-47a6-afca-ea82c37cdf81"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f1e3b34afc14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbase_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneratePositions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mshowBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcontFlag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-1a94c793e1db>\u001b[0m in \u001b[0;36mgeneratePositions\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# All possible N combination tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Using list comprehension + product()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'product' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "board = np.zeros((3, 3))\n",
        "base_positions = generatePositions(3)\n",
        "showBoard(board)\n",
        "contFlag=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybF2fVtCuGtC"
      },
      "outputs": [],
      "source": [
        "while contFlag:\n",
        "    row = int(input(\"Input your action row:\"))\n",
        "    col = int(input(\"Input your action col:\"))\n",
        "    user_position = (row, col)\n",
        "    board = updateAction(board, user_position, -1)\n",
        "    gameStatus = checkGameStatus(board)\n",
        "    if gameStatus !=0:\n",
        "      print(gameStatus)\n",
        "      contFlag=0\n",
        "    else:\n",
        "      upd_state = board.flatten()\n",
        "      ts_state = genTimeStep(upd_state)\n",
        "      action_step = policy.action(ts_state)\n",
        "      ag_action = action_step.action\n",
        "      ag_position = base_positions[ag_action]\n",
        "      board = updateAction(board, ag_position, 1)\n",
        "      showBoard(board)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TicTacToe-TFAgentTraining.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}